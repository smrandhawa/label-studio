{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"rel_info.json\",)\n",
    "\n",
    "info = json.load(f)\n",
    "f = open(\"dev.json\",)\n",
    "d = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#>>> d[0].keys()\n",
    "#[u'labels', u'sents', u'vertexSet', u'title']\n",
    "\n",
    "indexForData = 1\n",
    "AllSents = []\n",
    "\n",
    "for i,sent in enumerate(d[indexForData]['sents']):\n",
    "    print(i,' '.join(sent))\n",
    "    AllSents.append(' '.join(sent))\n",
    "\n",
    "for label in d[indexForData]['labels']:\n",
    "    if d[indexForData]['vertexSet'][label['h']][0]['type'] != \"ORG\" and d[indexForData]['vertexSet'][label['t']][0]['type'] != \"ORG\":\n",
    "        print(d[indexForData]['vertexSet'][label['h']])\n",
    "        print(d[indexForData]['vertexSet'][label['t']])\n",
    "        print(AllSents[d[indexForData]['vertexSet'][label['h']][0]['sent_id']])\n",
    "        print(AllSents[d[indexForData]['vertexSet'][label['t']][0]['sent_id']])\n",
    "        print(info[label['r']])\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexForData = 1\n",
    "abelssDict = {}\n",
    "for label in d[indexForData]['labels']:\n",
    "    if d[indexForData]['vertexSet'][label['h']][0]['type'] not in labelssDict:\n",
    "        labelssDict[d[indexForData]['vertexSet'][label['h']][0]['type']] = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import json\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def transferType(_type):\n",
    "    if _type == \"ORG\":\n",
    "        return [\"Organization\"]\n",
    "    elif _type == \"LOC\":\n",
    "        return [\"Location\"]\n",
    "    elif _type == \"TIME\":\n",
    "        return [\"Time\"]\n",
    "    elif _type == \"PER\":\n",
    "        return [\"Person\"]\n",
    "    elif _type == \"NUM\":\n",
    "        return [\"Number\"]\n",
    "    elif _type == \"MISC\":\n",
    "        return [\"Misc\"]\n",
    "\n",
    "# labelssDict = set()\n",
    "count = 0\n",
    "AllTasks = []\n",
    "for vertex in d:\n",
    "    AllSents = []\n",
    "    if count > 100:\n",
    "        break\n",
    "    # else:\n",
    "        # print(count)\n",
    "    # print(len(vertex['labels']))\n",
    "    if len(vertex['labels']) <= 3:\n",
    "        count = count + 1\n",
    "        for i,sent in enumerate(vertex['sents']):\n",
    "            # print(i,' '.join(sent))\n",
    "            AllSents.append(' '.join(sent)) \n",
    "        para = ' '.join(AllSents) \n",
    "        para = para.replace(\" .\", \".\")\n",
    "        # print(para)\n",
    "        task = {}\n",
    "        task['text'] = para\n",
    "        task['layout_id'] = 2\n",
    "        task['groundTruth'] = \" \"\n",
    "        task['format_type'] = 1\n",
    "        task['batch_id'] = 5\n",
    "        task['description'] = \" \"\n",
    "        task['id'] = count\n",
    "\n",
    "        comData = {}\n",
    "        comData['lead_time'] = 3.821\n",
    "        comData['result'] = []\n",
    "        comData['user'] = 0\n",
    "        # completion['created_at'] = 1616101190\n",
    "\n",
    "        foundLabesl = {}\n",
    "        foundRelations = {}\n",
    "\n",
    "        for label in vertex['labels']:\n",
    "            headent = vertex['vertexSet'][label['h']][0]\n",
    "            tailent = vertex['vertexSet'][label['t']][0]\n",
    "            # print(headent, tailent, label, info[label['r']])\n",
    "            # print(\"\\n\")\n",
    "            val1 = {}\n",
    "            if headent['name'] not in foundLabesl:                \n",
    "                val1['value'] = {}\n",
    "                val1['value']['start'] = para.find(headent['name'])\n",
    "                if val1['value']['start'] == -1:\n",
    "                    continue\n",
    "                val1['value']['end'] = val1['value']['start'] + len(headent['name'])\n",
    "                val1['value']['text'] = headent['name']\n",
    "                val1['value']['labels'] = transferType(headent['type'])\n",
    "                val1['id'] = id_generator(7)\n",
    "                val1['from_name'] = \"label\"\n",
    "                val1['to_name'] = \"text\"\n",
    "                val1['type'] = \"labels\"\n",
    "                comData['result'].append(val1)\n",
    "                foundLabesl[headent['name']] = val1['id']\n",
    "            else:\n",
    "                val1['id'] = foundLabesl[headent['name']]\n",
    "\n",
    "            val2 = {}\n",
    "            if tailent['name'] not in foundLabesl:\n",
    "                val2['value'] = {}\n",
    "                val2['value']['start'] = para.find(tailent['name'])\n",
    "                if val2['value']['start'] == -1:\n",
    "                    continue\n",
    "                val2['value']['end'] = val2['value']['start'] + len(tailent['name'])\n",
    "                val2['value']['text'] = tailent['name']\n",
    "                val2['value']['labels'] = transferType(tailent['type'])\n",
    "                val2['id'] = id_generator(7)\n",
    "                val2['from_name'] = \"label\"\n",
    "                val2['to_name'] = \"text\"\n",
    "                val2['type'] = \"labels\"\n",
    "                comData['result'].append(val2)\n",
    "                foundLabesl[tailent['name']] = val2['id']\n",
    "            else:\n",
    "                val2['id'] = foundLabesl[tailent['name']]\n",
    "\n",
    "            val3 = {}\n",
    "            if (val1['id'] in foundRelations and val2['id'] in foundRelations[val1['id']]) or (val2['id'] in foundRelations and val1['id'] in foundRelations[val2['id']]):\n",
    "                continue\n",
    "            else:\n",
    "                foundRelations[val1['id']] = val2['id']\n",
    "                foundRelations[val2['id']] = val1['id'] \n",
    "                val3['labels'] = [info[label['r']]]\n",
    "                val3['direction'] = \"bi\"\n",
    "                val3['from_id'] = val1['id']\n",
    "                val3['to_id'] = val2['id']\n",
    "                val3['type'] = \"relation\"\n",
    "                print(val3)\n",
    "                comData['result'].append(val3)\n",
    "                \n",
    "        completion = {}\n",
    "        completion['task_id'] =count\n",
    "        completion['user_id'] = 0\n",
    "        completion['data'] = json.dumps(comData)\n",
    "        completion['completed_at'] = 1616101190\n",
    "        completion['batch_id'] = 5\n",
    "        completion['was_skipped'] = 0\n",
    "        task['completion'] =  completion\n",
    "        AllTasks.append(task)   \n",
    "        # break\n",
    "todump = {}\n",
    "todump[\"tasks\"] = AllTasks\n",
    "# print(json.dumps(todump))\n",
    "with open('RETaskDaataWith1label.json', 'w') as outfile:\n",
    "    json.dump(todump, outfile)\n",
    "print(count, \"Done\")\n",
    "# print(completion)"
   ]
  },
  {
   "source": [
    "for i in labelssDict:\n",
    "    print(i)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "import string\n",
    "import random\n",
    "import json\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def transferType(_type):\n",
    "    if _type == \"ORG\":\n",
    "        return [\"Organization\"]\n",
    "    elif _type == \"LOC\":\n",
    "        return [\"Location\"]\n",
    "    elif _type == \"TIME\":\n",
    "        return [\"Time\"]\n",
    "    elif _type == \"PER\":\n",
    "        return [\"Person\"]\n",
    "    elif _type == \"NUM\":\n",
    "        return [\"Number\"]\n",
    "    elif _type == \"MISC\":\n",
    "        return [\"Misc\"]\n",
    "\n",
    "# labelssDict = set()\n",
    "count = 0\n",
    "AllTasks = []\n",
    "for vertex in d:\n",
    "    AllSents = []\n",
    "    if count < 100:\n",
    "        count = count + 1\n",
    "        continue\n",
    "    if count > 200:\n",
    "        break\n",
    "    # else:\n",
    "        # print(count)\n",
    "    # print(len(vertex['labels']))\n",
    "    # if len(vertex['labels']) <= 3:\n",
    "    for i,sent in enumerate(vertex['sents']):\n",
    "        # print(i,' '.join(sent))\n",
    "        AllSents.append(' '.join(sent)) \n",
    "    para = ' '.join(AllSents) \n",
    "    para = para.replace(\" .\", \".\")\n",
    "    # print(para)\n",
    "    task = {}\n",
    "    task['text'] = para\n",
    "    task['layout_id'] = 2\n",
    "    task['groundTruth'] = \" \"\n",
    "    task['format_type'] = 1\n",
    "    task['batch_id'] = 5\n",
    "    task['description'] = \" \"\n",
    "    task['id'] = count\n",
    "\n",
    "    comData = {}\n",
    "    comData['lead_time'] = 3.821\n",
    "    comData['result'] = []\n",
    "    comData['user'] = 0\n",
    "    # completion['created_at'] = 1616101190\n",
    "\n",
    "    foundLabesl = {}\n",
    "    foundRelations = {}\n",
    "\n",
    "    for label in vertex['labels']:\n",
    "        headent = vertex['vertexSet'][label['h']][0]\n",
    "        tailent = vertex['vertexSet'][label['t']][0]\n",
    "        # print(headent, tailent, label, info[label['r']])\n",
    "        # print(\"\\n\")\n",
    "        val1 = {}\n",
    "        if headent['name'] not in foundLabesl:                \n",
    "            val1['value'] = {}\n",
    "            val1['value']['start'] = para.find(headent['name'])\n",
    "            if val1['value']['start'] == -1:\n",
    "                continue\n",
    "            val1['value']['end'] = val1['value']['start'] + len(headent['name'])\n",
    "            val1['value']['text'] = headent['name']\n",
    "            val1['value']['labels'] = transferType(headent['type'])\n",
    "            val1['id'] = id_generator(7)\n",
    "            val1['from_name'] = \"label\"\n",
    "            val1['to_name'] = \"text\"\n",
    "            val1['type'] = \"labels\"\n",
    "            comData['result'].append(val1)\n",
    "            foundLabesl[headent['name']] = val1['id']\n",
    "        else:\n",
    "            val1['id'] = foundLabesl[headent['name']]\n",
    "\n",
    "        val2 = {}\n",
    "        if tailent['name'] not in foundLabesl:\n",
    "            val2['value'] = {}\n",
    "            val2['value']['start'] = para.find(tailent['name'])\n",
    "            if val2['value']['start'] == -1:\n",
    "                continue\n",
    "            val2['value']['end'] = val2['value']['start'] + len(tailent['name'])\n",
    "            val2['value']['text'] = tailent['name']\n",
    "            val2['value']['labels'] = transferType(tailent['type'])\n",
    "            val2['id'] = id_generator(7)\n",
    "            val2['from_name'] = \"label\"\n",
    "            val2['to_name'] = \"text\"\n",
    "            val2['type'] = \"labels\"\n",
    "            comData['result'].append(val2)\n",
    "            foundLabesl[tailent['name']] = val2['id']\n",
    "        else:\n",
    "            val2['id'] = foundLabesl[tailent['name']]\n",
    "\n",
    "        val3 = {}\n",
    "        if (val1['id'] in foundRelations and val2['id'] in foundRelations[val1['id']]) or (val2['id'] in foundRelations and val1['id'] in foundRelations[val2['id']]):\n",
    "            continue\n",
    "        else:\n",
    "            foundRelations[val1['id']] = val2['id']\n",
    "            foundRelations[val2['id']] = val1['id'] \n",
    "            val3['labels'] = [info[label['r']]]\n",
    "            val3['direction'] = \"bi\"\n",
    "            val3['from_id'] = val1['id']\n",
    "            val3['to_id'] = val2['id']\n",
    "            val3['type'] = \"relation\"\n",
    "            print(val3)\n",
    "            comData['result'].append(val3)\n",
    "            break\n",
    "            \n",
    "    completion = {}\n",
    "    completion['task_id'] =count\n",
    "    completion['user_id'] = 0\n",
    "    completion['data'] = json.dumps(comData)\n",
    "    completion['completed_at'] = 1616101190\n",
    "    completion['batch_id'] = 5\n",
    "    completion['was_skipped'] = 0\n",
    "    task['completion'] =  completion\n",
    "    count = count + 1\n",
    "    AllTasks.append(task)   \n",
    "        # break\n",
    "todump = {}\n",
    "todump[\"tasks\"] = AllTasks\n",
    "# print(json.dumps(todump))\n",
    "with open('Only1Relations.json', 'w') as outfile:\n",
    "    json.dump(todump, outfile)\n",
    "print(count, \"Done\")\n",
    "# print(completion)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import json\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "notFound = 0\n",
    "def transferType(_type):\n",
    "    if _type == \"ORG\":\n",
    "        return [\"Organization\"]\n",
    "    elif _type == \"LOC\":\n",
    "        return [\"Location\"]\n",
    "    elif _type == \"TIME\":\n",
    "        return [\"Time\"]\n",
    "    elif _type == \"PER\":\n",
    "        return [\"Person\"]\n",
    "    elif _type == \"NUM\":\n",
    "        return [\"Number\"]\n",
    "    elif _type == \"MISC\":\n",
    "        return [\"Misc\"]\n",
    "\n",
    "# labelssDict = set()\n",
    "count = 0\n",
    "AllTasks = []\n",
    "for vertex in d:\n",
    "    AllSents = []\n",
    "    if count < 200:\n",
    "        count = count + 1\n",
    "        continue\n",
    "    if count > 300:\n",
    "        break\n",
    "    # else:\n",
    "        # print(count)\n",
    "    # print(len(vertex['labels']))\n",
    "    # if len(vertex['labels']) <= 3:\n",
    "    for i,sent in enumerate(vertex['sents']):\n",
    "        # print(i,' '.join(sent))\n",
    "        AllSents.append(' '.join(sent)) \n",
    "    if len(AllSents) > 10 or len(AllSents) < 3:\n",
    "        continue\n",
    "    para = ' '.join(AllSents) \n",
    "    para = para.replace(\" .\", \".\")\n",
    "    para = para.replace(\" ,\", \",\")\n",
    "    para = para.replace(\" - \", \"-\")\n",
    "    para = para.replace(\" -\", \"-\")\n",
    "    para = para.replace(\"- \", \"-\")\n",
    "    para = para.replace(\" – \", \"–\")\n",
    "    para = para.replace(\" –\", \"–\")\n",
    "    para = para.replace(\"– \", \"–\")\n",
    "    para = para.replace(\" '\", \"'\")\n",
    "    para = para.replace(\" n't\", \"n't\")\n",
    "    para = para.replace(\" 're\", \"'re\")\n",
    "    para = para.replace(\"( \", \"(\")\n",
    "    para = para.replace(\" )\", \")\")\n",
    "    para = para.replace('\" ', '\"')\n",
    "    para = para.replace(' \"', '\"')\n",
    "    para = para.replace(' ;', ';')\n",
    "    para = para.replace(' :', ':')\n",
    "\n",
    "    # print(para)\n",
    "    task = {}\n",
    "    task['text'] = para\n",
    "    task['layout_id'] = 2\n",
    "    task['groundTruth'] = \" \"\n",
    "    task['format_type'] = 1\n",
    "    task['batch_id'] = 1\n",
    "    task['description'] = \" \"\n",
    "    task['id'] = count\n",
    "\n",
    "    comData = {}\n",
    "    comData['lead_time'] = 3.821\n",
    "    comData['result'] = []\n",
    "    comData['user'] = 0\n",
    "    # completion['created_at'] = 1616101190\n",
    "\n",
    "    foundLabesl = {}\n",
    "    foundRelations = {}\n",
    "\n",
    "    for label in vertex['labels']:\n",
    "        headent = vertex['vertexSet'][label['h']][0]\n",
    "        tailent = vertex['vertexSet'][label['t']][0]\n",
    "        # print(headent, tailent, label, info[label['r']])\n",
    "        # print(\"\\n\")\n",
    "        val1 = {}\n",
    "        if headent['name'] not in foundLabesl:                \n",
    "            val1['value'] = {}\n",
    "            headent['name'] = headent['name'].replace(\" .\", \".\")\n",
    "            headent['name'] = headent['name'].replace(\" ,\", \",\")\n",
    "            headent['name'] = headent['name'].replace(\" – \", \"–\")\n",
    "            headent['name'] = tailent['name'].replace(\" -\", \"–\")\n",
    "            headent['name'] = tailent['name'].replace(\"– \", \"–\")\n",
    "            headent['name'] = headent['name'].replace(\" - \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" -\", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\"- \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" '\", \"'\")\n",
    "            headent['name'] = headent['name'].replace(\" n't\", \"n't\")\n",
    "            headent['name'] = headent['name'].replace(\" 're\", \"'re\")\n",
    "            headent['name'] = headent['name'].replace(\"( \", \"(\")\n",
    "            headent['name'] = headent['name'].replace(\" )\", \")\")\n",
    "            headent['name'] = headent['name'].replace('\" ', '\"')\n",
    "            headent['name'] = headent['name'].replace(' \"', '\"')\n",
    "            headent['name'] = headent['name'].replace(' ;', ';')\n",
    "            headent['name'] = headent['name'].replace(' :', ':')\n",
    "            val1['value']['start'] = para.find(headent['name'])\n",
    "            if val1['value']['start'] == -1:\n",
    "                notFound = notFound + 1\n",
    "                print(headent['name'], transferType(headent['type']))\n",
    "                print(para)\n",
    "                continue\n",
    "            val1['value']['end'] = val1['value']['start'] + len(headent['name'])\n",
    "            val1['value']['text'] = headent['name']\n",
    "            val1['value']['labels'] = transferType(headent['type'])\n",
    "            val1['id'] = id_generator(7)\n",
    "            val1['from_name'] = \"label\"\n",
    "            val1['to_name'] = \"text\"\n",
    "            val1['type'] = \"labels\"\n",
    "            comData['result'].append(val1)\n",
    "            foundLabesl[headent['name']] = val1['id']\n",
    "        else:\n",
    "            val1['id'] = foundLabesl[headent['name']]\n",
    "\n",
    "        val2 = {}\n",
    "        if tailent['name'] not in foundLabesl:\n",
    "            val2['value'] = {}\n",
    "            tailent['name'] = tailent['name'].replace(\" .\", \".\")\n",
    "            tailent['name'] = tailent['name'].replace(\" ,\", \",\")\n",
    "            tailent['name'] = tailent['name'].replace(\" – \", \"–\")\n",
    "            tailent['name'] = tailent['name'].replace(\" -\", \"–\")\n",
    "            tailent['name'] = tailent['name'].replace(\"– \", \"–\")\n",
    "            tailent['name'] = tailent['name'].replace(\" - \", \"-\")\n",
    "            tailent['name'] = tailent['name'].replace(\" -\", \"-\")\n",
    "            tailent['name'] = tailent['name'].replace(\"- \", \"-\")\n",
    "            tailent['name'] = tailent['name'].replace(\" '\", \"'\")\n",
    "            tailent['name'] = tailent['name'].replace(\" n't\", \"n't\")\n",
    "            tailent['name'] = tailent['name'].replace(\" 're\", \"'re\")\n",
    "            tailent['name'] = tailent['name'].replace(\"( \", \"(\")\n",
    "            tailent['name'] = tailent['name'].replace(\" )\", \")\")\n",
    "            tailent['name'] = tailent['name'].replace('\" ', '\"')\n",
    "            tailent['name'] = tailent['name'].replace(' \"', '\"')\n",
    "            tailent['name'] = tailent['name'].replace(' ;', ';')\n",
    "            tailent['name'] = tailent['name'].replace(' :', ':')\n",
    "            val2['value']['start'] = para.find(tailent['name'])\n",
    "            if val2['value']['start'] == -1:\n",
    "                notFound = notFound + 1\n",
    "                print(tailent['name'], transferType(tailent['type']))\n",
    "                print(para)\n",
    "                continue\n",
    "            val2['value']['end'] = val2['value']['start'] + len(tailent['name'])\n",
    "            val2['value']['text'] = tailent['name']\n",
    "            val2['value']['labels'] = transferType(tailent['type'])\n",
    "            val2['id'] = id_generator(7)\n",
    "            val2['from_name'] = \"label\"\n",
    "            val2['to_name'] = \"text\"\n",
    "            val2['type'] = \"labels\"\n",
    "            comData['result'].append(val2)\n",
    "            foundLabesl[tailent['name']] = val2['id']\n",
    "        else:\n",
    "            val2['id'] = foundLabesl[tailent['name']]\n",
    "\n",
    "        # val3 = {}\n",
    "        # if (val1['id'] in foundRelations and val2['id'] in foundRelations[val1['id']]) or (val2['id'] in foundRelations and val1['id'] in foundRelations[val2['id']]):\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     foundRelations[val1['id']] = val2['id']\n",
    "        #     foundRelations[val2['id']] = val1['id'] \n",
    "        #     val3['labels'] = [info[label['r']]]\n",
    "        #     val3['direction'] = \"bi\"\n",
    "        #     val3['from_id'] = val1['id']\n",
    "        #     val3['to_id'] = val2['id']\n",
    "        #     val3['type'] = \"relation\"\n",
    "        #     print(val3)\n",
    "        #     comData['result'].append(val3)\n",
    "        #     break\n",
    "            \n",
    "    completion = {}\n",
    "    completion['task_id'] =count\n",
    "    completion['user_id'] = 0\n",
    "    completion['data'] = json.dumps(comData)\n",
    "    completion['completed_at'] = 1616101190\n",
    "    completion['batch_id'] = 1\n",
    "    completion['was_skipped'] = 0\n",
    "    task['completion'] =  completion\n",
    "    count = count + 1\n",
    "    AllTasks.append(task)   \n",
    "        # break\n",
    "todump = {}\n",
    "todump[\"tasks\"] = AllTasks\n",
    "# print(json.dumps(todump))\n",
    "with open('OnlyEntitiesNew.json', 'w') as outfile:\n",
    "    json.dump(todump, outfile)\n",
    "print(count, \"Done\")\n",
    "print(notFound, \"Done\")\n",
    "# print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "David Edgerton ['Person'] NotFound\nfoundLabesl Pos David Edgerton 80\nJames Whitman\"Jim\"McLamore (May 30, 1926-August 9, 1996) was with David Edgerton responsible for the expansion of the Burger King fast food franchise. McLamore attended Northfield Mount Hermon School before matriculating at Cornell University. McLamore was an employee and also a businessman before. Edgerton originally opened Insta Burger King in Miami, Florida on March 1, 1954. Three months later, on June 1, he met McLamore and they formed the Burger King Corporation. The corporation opened Burger King stores and went on to introduce the Whopper burger in 1957, when it also dropped\"Insta\"from the name. The pair sold the business to Pillsbury in 1967 and McLamore served as Burger King's president until 1970, and was chairman to 1976. McLamore died of cancer in Coral Gables, Florida on August 9, 1996, at the age of 70.\nBillboard 200 ['Misc'] NotFound\nfoundLabesl Pos Billboard 200 631\nPowerglide is the second album by the American band the New Riders of the Purple Sage. The music is a psychedelic hybrid of country rock, and includes guest musicians Jerry Garcia and Bill Kreutzmann from the Grateful Dead, along with noted session player Nicky Hopkins. The album contains six original tunes by the band, plus covers such as\"I Don't Need No Doctor\",\"Hello Mary Lou\", and\"Willie and the Hand Jive\". Powerglide was the first New Riders album to feature Buddy Cage, who had replaced Garcia as the New Riders' pedal steel guitar player. It was the band's highest-charting album, reaching number 33 on the Billboard 200.\nNever Gonna Give You Up ['Misc'] NotFound\nfoundLabesl Pos Never Gonna Give You Up 0\n\"Cry for Help\"is the title of the first single taken from British dance-pop singer Rick Astley's third studio album, Free. It was written by Rick Astley and Rob Fisher. The Andraé Crouch Choir provided backing vocals. Released as a single in January 1991,\"Cry for Help\"reached number seven on both the UK Singles Chart and the US Billboard Hot 100. It also reached number four in Canada and was a number-one hit on both the US and Canadian Adult Contemporary charts. The song's number-seven UK chart placing meant that Astley became the first male solo artist to have his first eight singles reach the British top 10. The song is a soul ballad, unlike Astley's other more dance-oriented hit singles such as\"Never Gon na Give You Up\". It was co-written by British singer Rob Fisher, formerly one half of the 1980s pop outfits Naked Eyes and Climie Fisher. To date, the song was Astley's last appearance in the top 10 in either the US or UK. The song has been sung by Rick Astley in duet with Soren Sko and covered by Thomas Anders (ex-Modern Talking).\nFatih Terim ['Person'] NotFound\nfoundLabesl Pos Fatih Terim 11\nFatih Terim, Commendatore OSSI, T.C, (born 4 September 1953) is a Turkish association football manager and former player. He is currently the manager of Galatasaray, a position he previously held three times. Terim has managed several clubs in Italy and Turkey, as well as the Turkish national football team, most recently from 2013 to 2017. In a survey conducted by the International Federation of Football History & Statistics (IFFHS) in 80 countries, he was placed among the best eight managers in the world, receiving his award at a ceremony held in Rothenburg, Germany, on 8 January 2001. Terim received a nomination for UEFA manager of the year 2008, and Eurosport named him the best coach at UEFA Euro 2008. In December 2008, he was ranked the seventh-best football manager in the world by World Soccer Magazine in 2008. His Turkish nickname is\"İmparator\", and his Italian nickname is\"Imperatore\". Both names mean\"emperor\".\nIFFHS ['Organization'] NotFound\nfoundLabesl Pos IFFHS 435\nFatih Terim, Commendatore OSSI, T.C, (born 4 September 1953) is a Turkish association football manager and former player. He is currently the manager of Galatasaray, a position he previously held three times. Terim has managed several clubs in Italy and Turkey, as well as the Turkish national football team, most recently from 2013 to 2017. In a survey conducted by the International Federation of Football History & Statistics (IFFHS) in 80 countries, he was placed among the best eight managers in the world, receiving his award at a ceremony held in Rothenburg, Germany, on 8 January 2001. Terim received a nomination for UEFA manager of the year 2008, and Eurosport named him the best coach at UEFA Euro 2008. In December 2008, he was ranked the seventh-best football manager in the world by World Soccer Magazine in 2008. His Turkish nickname is\"İmparator\", and his Italian nickname is\"Imperatore\". Both names mean\"emperor\".\n£ 940 million ['Number'] NotFound\nfoundLabesl Pos £ 940 million 530\nUniversity College London Hospitals NHS Foundation Trust (UCLH) is an NHS foundation trust based in London, United Kingdom. It comprises University College Hospital, University College Hospital at Westmoreland Street, the UCH Macmillan Cancer Centre, the Eastman Dental Hospital, the Hospital for Tropical Diseases, the National Hospital for Neurology and Neurosurgery, the Royal London Hospital for Integrated Medicine and the Royal National Throat, Nose and Ear Hospital. The Trust has an annual turnover of around £ 940 million and employs approximately 8,180 staff. Each year its hospitals treat over 500,000 outpatients appointments and admit over 100,000 patients. In partnership with University College London, UCLH has major research activities is part of the UCLH/UCL Biomedical Research Centre and the UCL Partners academic health science centre. Its hospitals are also major teaching centres and offer training for nurses, doctors and other health care professionals in partnership with City, University of London, Kings College London, London South Bank University and UCL Medical School.\nMuir Beach ['Location'] NotFound\nfoundLabesl Pos Muir Beach 10\nMuir Beach is a census designated place (CDP), unincorporated community, and beach that is located northwest of San Francisco in western Marin County, California, United States. It is named for John Muir. The population was 310 at the 2010 census. The community itself flanks the northwest side of the beach. Located about 2 miles (3   km) from the entrance to Muir Woods National Monument, the beach is about 1000 feet (305 m) long and 200 feet (61 m) wide, with coarse sand and several large boulders. Redwood Creek empties into the beach. There is a parking lot at the beach, which is accessible via a footbridge. The beach was formerly called Big Lagoon after a freshwater lagoon that was located where the parking lot is now. Damage from 20th century dairy farms interfered with the flow of the creek and the lagoon.\n14 nm ['Number'] NotFound\nfoundLabesl Pos 14 nm 0\nSkylake is the codename used by Intel for a processor microarchitecture that was launched in August 2015 succeeding the Broadwell microarchitecture. Skylake is a microarchitecture redesign using the same 14   nm manufacturing process technology as its predecessor, serving as a\"tock\"in Intel's\"tick-tock\"manufacturing and design model. According to Intel, the redesign brings greater CPU and GPU performance and reduced power consumption. Skylake CPUs share its microarchitecture with Kaby Lake, Coffee Lake and Cannon Lake CPUs. Skylake is the last Intel platform on which Windows earlier than Windows 10 will be officially supported by Microsoft, although enthusiast-created modifications exist that allow Windows 8.1 and earlier to continue to receive updates on later platforms. Some of the processors based on the Skylake microarchitecture are marketed as\"6th-generation Core\". On October 8, 2018, Intel announced new 9th gen Core X 98xx/99xx series CPUs, with the Core i9-9980XE Extreme Edition leading the launch.\nThe Eminem Show ['Misc'] NotFound\nfoundLabesl Pos The Eminem Show 313\nThe Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. The Eminem Show includes the commercially successful singles\"Without Me\",\"Cleanin' Out My Closet\",\"Superman\", and\"Sing for the Moment\". The Eminem Show reached number one in nineteen countries, including Australia, Canada, the United Kingdom and the United States, and was the best-selling album of 2002 in the United States, with 7,600,000 copies sold. Since its release in 2002, the album has sold 10,600,000 copies in the United States and over 30 million copies worldwide. At the 2003 Grammy Awards, it was nominated for Album of the Year and became Eminem's third album in four years to win the award for Best Rap Album. On March 7, 2011, the album was certified 10× Platinum (Diamond) by the RIAA, making it Eminem's second album to go Diamond in the United States.\nAmerican ['Location'] NotFound\nfoundLabesl Pos American 54\nThe Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. The Eminem Show includes the commercially successful singles\"Without Me\",\"Cleanin' Out My Closet\",\"Superman\", and\"Sing for the Moment\". The Eminem Show reached number one in nineteen countries, including Australia, Canada, the United Kingdom and the United States, and was the best-selling album of 2002 in the United States, with 7,600,000 copies sold. Since its release in 2002, the album has sold 10,600,000 copies in the United States and over 30 million copies worldwide. At the 2003 Grammy Awards, it was nominated for Album of the Year and became Eminem's third album in four years to win the award for Best Rap Album. On March 7, 2011, the album was certified 10× Platinum (Diamond) by the RIAA, making it Eminem's second album to go Diamond in the United States.\n9.2 km ['Number'] NotFound\nfoundLabesl Pos 9.2 km 0\nDurgada is a rural village in Gollaprolu mandal, East Godavari district, Andhra Pradesh, India. The village was formerly known as durga ooda, durga vaahini. It is located north-east to the Pithapuram and [ Gollaprolu ]. The village is located 1.8 kilometers away from NH 214 and 3 kilometers away from NH 5. The nearest city (35   km) is Kakinada. The most convenient way of travel for the people in village is by train. Durgada has a railway gate halt. Other than this the nearest railway stations are Ravikampadu East Godavari (2.6   km) and Gollaprolu (9.2   km) and nearest railway junction is Samalkot. Nearest air strip is Madhurapudi, Rajahmundry (75   km) and nearest airport is Vishakapatnam (125   km) and nearest seaport is Kakinada Port.\n75 km ['Number'] NotFound\nfoundLabesl Pos 75 km 0\nDurgada is a rural village in Gollaprolu mandal, East Godavari district, Andhra Pradesh, India. The village was formerly known as durga ooda, durga vaahini. It is located north-east to the Pithapuram and [ Gollaprolu ]. The village is located 1.8 kilometers away from NH 214 and 3 kilometers away from NH 5. The nearest city (35   km) is Kakinada. The most convenient way of travel for the people in village is by train. Durgada has a railway gate halt. Other than this the nearest railway stations are Ravikampadu East Godavari (2.6   km) and Gollaprolu (9.2   km) and nearest railway junction is Samalkot. Nearest air strip is Madhurapudi, Rajahmundry (75   km) and nearest airport is Vishakapatnam (125   km) and nearest seaport is Kakinada Port.\n301 Done\n12 Done\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "notFound = 0\n",
    "def transferType(_type):\n",
    "    if _type == \"ORG\":\n",
    "        return [\"Organization\"]\n",
    "    elif _type == \"LOC\":\n",
    "        return [\"Location\"]\n",
    "    elif _type == \"TIME\":\n",
    "        return [\"Time\"]\n",
    "    elif _type == \"PER\":\n",
    "        return [\"Person\"]\n",
    "    elif _type == \"NUM\":\n",
    "        return [\"Number\"]\n",
    "    elif _type == \"MISC\":\n",
    "        return [\"Misc\"]\n",
    "\n",
    "# labelssDict = set()\n",
    "count = 0\n",
    "AllTasks = []\n",
    "shouldBreak = False\n",
    "for vertex in d:\n",
    "    AllSents = []\n",
    "    if count < 200:\n",
    "        count = count + 1\n",
    "        continue\n",
    "    if count > 300:\n",
    "        break\n",
    "    # else:\n",
    "        # print(count)\n",
    "    # print(vertex['vertexSet'])\n",
    "    # if len(vertex['labels']) <= 3:\n",
    "    for i,sent in enumerate(vertex['sents']):\n",
    "        # print(i,' '.join(sent))\n",
    "        s = ' '.join(sent)\n",
    "        s = s.replace('   ', ' ')\n",
    "        AllSents.append(s) \n",
    "    \n",
    "    if len(AllSents) > 10 or len(AllSents) < 3:\n",
    "        continue\n",
    "    para = ' '.join(AllSents) \n",
    "    para = para.replace(\" .\", \".\")\n",
    "    para = para.replace(\" ,\", \",\")\n",
    "    para = para.replace(\" - \", \"-\")\n",
    "    para = para.replace(\" -\", \"-\")\n",
    "    para = para.replace(\"- \", \"-\")\n",
    "    para = para.replace(\" – \", \"-\")\n",
    "    para = para.replace(\" –\", \"-\")\n",
    "    para = para.replace(\"– \", \"-\")\n",
    "    para = para.replace(\"–\", \"-\")\n",
    "    para = para.replace(\" '\", \"'\")\n",
    "    para = para.replace(\" n't\", \"n't\")\n",
    "    para = para.replace(\" 're\", \"'re\")\n",
    "    para = para.replace(\"( \", \"(\")\n",
    "    para = para.replace(\" )\", \")\")\n",
    "    para = para.replace(\"[ \", \"[\")\n",
    "    para = para.replace(\" ]\", \"]\")\n",
    "    para = para.replace('\" ', '\"')\n",
    "    para = para.replace(' \"', '\"')\n",
    "    para = para.replace(' ;', ';')\n",
    "    para = para.replace(' :', ':')\n",
    "    para = para.replace(' %', '%')\n",
    "    para = para.replace('% ', '%')\n",
    "    # para = para.replace(' $', '$')\n",
    "    para = para.replace('$ ', '$')\n",
    "    para = para.replace('   ', ' ')    \n",
    "    para = re.sub(r' / ', '/', para)\n",
    "    para = re.sub(r'  *', ' ', para)\n",
    "    \n",
    "    # print(para)\n",
    "    task = {}\n",
    "    task['text'] = para\n",
    "    task['layout_id'] = 2\n",
    "    task['groundTruth'] = \" \"\n",
    "    task['format_type'] = 1\n",
    "    task['batch_id'] = 1\n",
    "    task['description'] = \" \"\n",
    "    task['id'] = count\n",
    "\n",
    "    comData = {}\n",
    "    comData['lead_time'] = 3.821\n",
    "    comData['result'] = []\n",
    "    comData['user'] = 0\n",
    "    # completion['created_at'] = 1616101190\n",
    "\n",
    "    foundLabesl = {}\n",
    "    foundLabeslCount = {}\n",
    "    foundRelations = {}\n",
    "\n",
    "    for vertexset in vertex['vertexSet']:\n",
    "        # headent = vertex['vertexSet'][label['h']][0]\n",
    "        # tailent = vertex['vertexSet'][label['t']][0]\n",
    "        # print(headent, tailent, label, info[label['r']])\n",
    "        # print(\"\\n\")\n",
    "        for headent in vertexset:\n",
    "            val1 = {}\n",
    "            val1['value'] = {}\n",
    "            headent['name'] = headent['name'].replace(\" .\", \".\")\n",
    "            headent['name'] = headent['name'].replace(\" ,\", \",\")\n",
    "            headent['name'] = headent['name'].replace(\" – \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" –\", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\"– \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\"–\", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" - \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" -\", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\"- \", \"-\")\n",
    "            headent['name'] = headent['name'].replace(\" '\", \"'\")\n",
    "            headent['name'] = headent['name'].replace(\" n't\", \"n't\")\n",
    "            headent['name'] = headent['name'].replace(\" 're\", \"'re\")\n",
    "            headent['name'] = headent['name'].replace(\"( \", \"(\")\n",
    "            headent['name'] = headent['name'].replace(\" )\", \")\")\n",
    "            headent['name'] = headent['name'].replace(\"[ \", \"[\")\n",
    "            headent['name'] = headent['name'].replace(\" ]\", \"]\")\n",
    "            headent['name'] = headent['name'].replace('\" ', '\"')\n",
    "            headent['name'] = headent['name'].replace(' \"', '\"')\n",
    "            headent['name'] = headent['name'].replace(' ;', ';')\n",
    "            headent['name'] = headent['name'].replace(' :', ':')\n",
    "            headent['name'] = headent['name'].replace(' %', '%')\n",
    "            headent['name'] = headent['name'].replace('% ', '%')\n",
    "            headent['name'] = headent['name'].replace('$ ', '$')\n",
    "            # headent['name'] = headent['name'].replace(' $', '$')            \n",
    "            headent['name'] = headent['name'].replace('   ', ' ')            \n",
    "            headent['name'] = re.sub(r' / ', '/', headent['name'])\n",
    "            headent['name'] = re.sub(r'  *', ' ', headent['name'])\n",
    "            \n",
    "            if headent['name'] not in foundLabesl:        \n",
    "                foundLabesl[headent['name']] = 0\n",
    "            nextPos = foundLabesl[headent['name']]\n",
    "            val1['value']['start'] = para.find(headent['name'], nextPos)\n",
    "            # print (val1['value']['start'], headent['name'], nextPos)\n",
    "                        \n",
    "            if val1['value']['start'] == -1:\n",
    "                notFound = notFound + 1\n",
    "                print(headent['name'], transferType(headent['type']), \"NotFound\")\n",
    "                print(\"foundLabesl Pos\",headent['name'],foundLabesl[headent['name']])\n",
    "                print(para)\n",
    "                continue\n",
    "                # break\n",
    "            foundLabesl[headent['name']] =  val1['value']['start'] + len(headent['name'])\n",
    "            # print(\"foundLabesl Pos\",headent['name'],foundLabesl[headent['name']])\n",
    "            # shouldBreak = True\n",
    "\n",
    "            val1['value']['end'] = val1['value']['start'] + len(headent['name'])\n",
    "            val1['value']['text'] = headent['name']\n",
    "            val1['value']['labels'] = transferType(headent['type'])\n",
    "            val1['id'] = id_generator(7)\n",
    "            val1['from_name'] = \"label\"\n",
    "            val1['to_name'] = \"text\"\n",
    "            val1['type'] = \"labels\"\n",
    "            comData['result'].append(val1)\n",
    "       \n",
    "    completion = {}\n",
    "    completion['task_id'] =count\n",
    "    completion['user_id'] = 0\n",
    "    completion['data'] = json.dumps(comData)\n",
    "    completion['completed_at'] = 1616101190\n",
    "    completion['batch_id'] = 1\n",
    "    completion['was_skipped'] = 0\n",
    "    task['completion'] =  completion\n",
    "    count = count + 1\n",
    "    AllTasks.append(task)   \n",
    "    # if shouldBreak:        \n",
    "    #     break\n",
    "todump = {}\n",
    "todump[\"tasks\"] = AllTasks\n",
    "# print(json.dumps(todump))\n",
    "with open('OnlyEntitiesNew.json', 'w') as outfile:\n",
    "    json.dump(todump, outfile)\n",
    "print(count, \"Done\")\n",
    "print(notFound, \"Done\")\n",
    "# print(AllTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}